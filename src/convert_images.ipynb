{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data conversion and normalization\n",
    "\n",
    "This file contains all the code necessary to convert the [NYU hand pose dataset](http://cims.nyu.edu/~tompson/NYU_Hand_Pose_Dataset.htm) into a single [.hdf5](http://www.h5py.org/) file for use by [deep_hand_pose.ipynb](deep_hand_pose.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.misc\n",
    "import scipy.io as sio\n",
    "from keras.utils.generic_utils import Progbar\n",
    "from PIL import Image\n",
    "import sys\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset\n",
    "\n",
    "The default dataset output directory is the [dataset](../dataset) subfolder of the project root.\n",
    "\n",
    "The default input dataset directory, containing the NYU hand pose dataset, is assumed to be a folder called [nyu_hand_dataset_v2](../nyu_hand_dataset_v2) in the project root.\n",
    "\n",
    "Simply unzipping the NYU dataset into the project root should be enough to get up and running without any extra configuration.\n",
    "\n",
    "#### Training data\n",
    "\n",
    "Training data is located in the [dataset/train](../nyu_hand_dataset_v2/dataset/train) subfolder of the NYU dataset.\n",
    "\n",
    "There are 72757 training images and corresponding labels.\n",
    "\n",
    "#### Testing data\n",
    "\n",
    "Testing data is located in the [dataset/test](../nyu_hand_dataset_v2/dataset/test) subfolder of the NYU dataset.\n",
    "\n",
    "There are 8252 testing images and corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "INPUT_DIR   = '../nyu_hand_dataset_v2/dataset'\n",
    "DATASET_DIR = '../dataset'\n",
    "dataset     = h5py.File(path.join(DATASET_DIR, 'dataset.hdf5'))\n",
    "\n",
    "data_path   = lambda *args : path.join(INPUT_DIR, *args)\n",
    "image_path  = lambda type, angle, index : '%s_%d_%07d.png' % (type, angle, index + 1)\n",
    "labels_path = 'joint_data.mat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data(set, type, angle, dtype='uint8'):\n",
    "    labels = load_labels(set, angle)\n",
    "    image_paths = (data_path(set, image_path(type, angle, i)) for i in range(len(labels)))\n",
    "    images = load_images(image_paths, dtype)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Images\n",
    "\n",
    "Each image is $ 640 \\times 480 \\times 3 $, stored in the standard .png format as unsigned 8 bit integers.\n",
    "\n",
    "Images are named as [type]\\_[angle]\\_[number].png.\n",
    "\n",
    "Type may be one of:\n",
    "\n",
    "* rgb\n",
    "* depth\n",
    "* synthdepth\n",
    "\n",
    "Angle may be one of:\n",
    "\n",
    "* 1 (front)\n",
    "* 2 (top)\n",
    "* 3 (side)\n",
    "\n",
    "Numbers start at 1 and are padded with zeroes to 7 digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_images(image_paths, dtype='uint8'):\n",
    "    for image_path in image_paths:\n",
    "        with Image.open(image_path) as image:\n",
    "            yield np.asarray(image, dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labels\n",
    "\n",
    "Each label is $ 36 \\times 3 $, stored in the MATLAB .mat format as double precision floating point numbers in a file named joint_data.mat. There is one label for each of the three camera angles.\n",
    "\n",
    "Each 3-tuple is a point in the $ uvd $ coordinate space, which is the same as that used in the [depth images](#Depth-images). $ u $ and $ v $ are in pixels, while $ d $ is in millimeters.\n",
    "\n",
    "There are 36 tuples per label, corresponding to keypoints on the hand. Each finger is represented by 6 keypoints, plus three for the palm and three for the wrist.\n",
    "\n",
    "Finger keypoints are named as [finger]\\_[segment]\\_[component].\n",
    "\n",
    "Finger may be one of:\n",
    "\n",
    "* F1 (pinky)\n",
    "* F2 (ring)\n",
    "* F3 (middle)\n",
    "* F4 (index)\n",
    "* TH (thumb)\n",
    "\n",
    "Segment may be one of:\n",
    "\n",
    "* KNU3 (fingertip / distal phalanx)\n",
    "* KNU2 (finger middle / middle phalanx)\n",
    "* KNU1 (finger base / proximal phalanx)\n",
    "\n",
    "Component may be one of:\n",
    "\n",
    "* A (bone neck)\n",
    "* B (joint / bone base)\n",
    "\n",
    "Palm and wrist keypoints are named as follows:\n",
    "\n",
    "* PALM_1 (outer / ulnar wrist)\n",
    "* PALM_2 (inner / radial wrist)\n",
    "* PALM_3 (inner / radial palm)\n",
    "* PALM_4 (outer / ulnar palm)\n",
    "* PALM_5 (middle palm / ring metacarpal)\n",
    "* PALM_6 (middle wrist / lunate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_labels(set, angle):\n",
    "    joint_data  = sio.loadmat(data_path(set, labels_path))\n",
    "    labels      = joint_data['joint_uvd'][angle - 1]\n",
    "    joint_names = [name[0] for name in joint_data['joint_names'][0]]\n",
    "\n",
    "    return pd.Panel(labels, major_axis=joint_names, minor_axis=['u', 'v', 'd'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Depth images\n",
    "\n",
    "Depth images and synthetic depth images [store the most significant bits of depth data in the green channel and the least significant bits in the blue channel](https://github.com/jsupancic/deep_hand_pose/blob/master/src/caffe/jvl.cpp#L507)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_depth(image):\n",
    "    return np.expand_dims((image[:, :, 1] << 8) + image[:, :, 2], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "The [Deep Hand Pose](https://github.com/jsupancic/deep_hand_pose) project performs a number of normalization steps on the NYU dataset before it is fed into the model.\n",
    "\n",
    "Broadly, these consist of:\n",
    "\n",
    "* [Rescaling image depth](https://github.com/jsupancic/deep_hand_pose/blob/master/src/caffe/jvl.cpp#L510) from millimeters to centimeters\n",
    "* [Rescaling label depth](https://github.com/jsupancic/deep_hand_pose/blob/master/src/caffe/jvl.cpp#L621) from millimeters to centimeters\n",
    "* [Recentering](https://github.com/jsupancic/deep_hand_pose/blob/master/src/caffe/jvl.cpp#L631) around the [middle finger base](https://github.com/jsupancic/deep_hand_pose/blob/master/src/caffe/jvl.cpp#L630)\n",
    "* [Clipping](https://github.com/jsupancic/deep_hand_pose/blob/master/src/caffe/jvl.cpp#L650) to [a smaller, depth-dependent bounding box](https://github.com/jsupancic/deep_hand_pose/blob/master/src/caffe/jvl.cpp#L638)\n",
    "* [Clamping](https://github.com/jsupancic/deep_hand_pose/blob/master/src/caffe/jvl.cpp#L665) depth data and scaling it to a range of -1 to 1\n",
    "* [Resizing](https://github.com/jsupancic/deep_hand_pose/blob/master/src/caffe/layers/hand_data_layer.cpp#L422) the image and label to 128 x 128\n",
    "* [Extracting a subset of the hand pose keypoints](https://github.com/jsupancic/deep_hand_pose/blob/master/include/jvl/blob_io.hpp#L8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(image, label):\n",
    "    label = label.copy()\n",
    "    \n",
    "    label.ix[:, 'd'] /= 10.\n",
    "    image = image / 10.\n",
    "    \n",
    "    center = label.ix['F3_KNU1_B', ['v', 'u']]\n",
    "    center_depth = label.ix[:, 'd'].median()\n",
    "    \n",
    "    # TODO: what do these mean\n",
    "    metric_size = 38\n",
    "    fx = 525\n",
    "    metric_size_factor = fx * metric_size\n",
    "    \n",
    "    bounding_box = np.array([[0, 0], [1, 1]], dtype='float')\n",
    "    bounding_box = (bounding_box - 0.5) * metric_size_factor / center_depth\n",
    "    bounding_box += center\n",
    "    \n",
    "    label.ix[:, 'd'] -= center_depth\n",
    "    \n",
    "    image, label = clip(image, label, bounding_box.astype(int))\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Resize an image to the specified dimensions, scaling its label accordingly\n",
    "def resize(image, label, dimensions):\n",
    "    label.ix[:, ['v', 'u']] *= np.array(dimensions) / image.shape[:-1]\n",
    "    \n",
    "    # TODO: Try to implement or use OpenCV's INTER_AREA resize strategy?\n",
    "    image = scipy.misc.imresize(np.squeeze(image), dimensions, 'bilinear')\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clip an image to the specified bounding box, translating its label accordingly\n",
    "# Bounding box should look like np.array([[x_1, y_1], [x_2, y_2]]), where\n",
    "# (x_1, y_1) are the coordinates of the lower left corner and \n",
    "# (x_2, y_2) are the coordinates of the upper right corner\n",
    "def clip(image, label, bounding_box):\n",
    "    label.ix[:, ['v', 'u']] -= bounding_box[0]\n",
    "    \n",
    "    image_box = np.array([[0, 0], image.shape[:-1]], dtype='int')\n",
    "    \n",
    "    padding = np.array([image_box[0] - bounding_box[0], bounding_box[1] - image_box[1]]).clip(0)\n",
    "    bounding_box += padding[0]\n",
    "    padding = np.concatenate((padding.T, np.array([[0, 0]])))\n",
    "    \n",
    "    image = np.pad(image, padding, 'edge')\n",
    "    image = image[slice(*bounding_box[:, 0]), slice(*bounding_box[:, 1])]\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1254/72757 [..............................] - ETA: 1822s"
     ]
    }
   ],
   "source": [
    "images, labels = load_data('train', 'depth', 1)\n",
    "\n",
    "data_image = dataset.require_dataset('image/train', (len(labels), 128, 128, 1), dtype='float')\n",
    "data_label = dataset.require_dataset('label/train', (len(labels), 28), dtype='float')\n",
    "\n",
    "p = Progbar(len(labels))\n",
    "\n",
    "for image, item in zip(images, labels.iteritems()):\n",
    "    index, label = item\n",
    "    p.update(index)\n",
    "    \n",
    "    image = convert_depth(image)\n",
    "    image, label = normalize(image, label)\n",
    "    image, label = resize(image, label, (128, 128))\n",
    "    \n",
    "    data_image[index] = np.expand_dims(image, 2)\n",
    "    data_label[index] = label.ix[[\n",
    "            'F1_KNU3_A',\n",
    "            'F1_KNU2_B',\n",
    "            'F2_KNU3_A',\n",
    "            'F2_KNU2_B',\n",
    "            'F3_KNU3_A',\n",
    "            'F3_KNU2_B',\n",
    "            'F4_KNU3_A',\n",
    "            'F4_KNU2_B',\n",
    "            'TH_KNU3_A',\n",
    "            'TH_KNU3_B',\n",
    "            'TH_KNU2_B',\n",
    "            'PALM_1',\n",
    "            'PALM_2',\n",
    "            'PALM_3'\n",
    "        ], ['u', 'v']].values.flatten()\n",
    "\n",
    "dataset.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
