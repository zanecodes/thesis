{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "This file contains all the code necessary to convert the [NYU hand pose dataset](http://cims.nyu.edu/~tompson/NYU_Hand_Pose_Dataset.htm) into a single [.hdf5](http://www.h5py.org/) file for use by [model.ipynb](model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.misc\n",
    "import scipy.io as sio\n",
    "from keras.utils.generic_utils import Progbar\n",
    "from PIL import Image\n",
    "import sys\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input dataset\n",
    "\n",
    "The default input dataset directory, containing the NYU hand pose dataset, is assumed to be a folder called nyu_hand_dataset_v2/ in the project root.\n",
    "\n",
    "Simply unzipping the NYU dataset into the project root should be enough to get up and running without any extra configuration.\n",
    "\n",
    "#### Training data\n",
    "\n",
    "Training data is located in the dataset/train/ subfolder of the NYU dataset.\n",
    "\n",
    "There are 72757 training images and corresponding labels.\n",
    "\n",
    "#### Testing data\n",
    "\n",
    "Testing data is located in the dataset/test/ subfolder of the NYU dataset.\n",
    "\n",
    "There are 8252 testing images and corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "INPUT_DIR   = '../nyu_hand_dataset_v2/dataset'\n",
    "data_path   = lambda *args : path.join(INPUT_DIR, *args)\n",
    "image_path  = lambda type, angle, index : '%s_%d_%07d.png' % (type, angle, index + 1)\n",
    "labels_path = 'joint_data.mat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data(set, type, angle, dtype='uint16'):\n",
    "    labels = load_labels(set, angle)\n",
    "    image_paths = (data_path(set, image_path(type, angle, i)) for i in range(len(labels)))\n",
    "    images = load_images(image_paths, dtype)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Images\n",
    "\n",
    "Each image is $ 640 \\times 480 \\times 3 $, stored in the standard .png format as unsigned 8 bit integers.\n",
    "\n",
    "Images contain either RGB, depth, or synthetic depth data, from a front, top, or side viewing angle.\n",
    "\n",
    "Images are named as [type]\\_[angle]\\_[number].png.\n",
    "\n",
    "Type may be one of:\n",
    "\n",
    "* rgb\n",
    "* depth\n",
    "* synthdepth\n",
    "\n",
    "Angle may be one of:\n",
    "\n",
    "* 1 (front)\n",
    "* 2 (top)\n",
    "* 3 (side)\n",
    "\n",
    "Numbers start at 1 and are padded with zeroes to 7 digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_images(image_paths, dtype='uint16'):\n",
    "    for image_path in image_paths:\n",
    "        with Image.open(image_path) as image:\n",
    "            yield np.asarray(image, dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labels\n",
    "\n",
    "Each label is $ 36 \\times 3 $, stored in the MATLAB .mat format as double precision floating point numbers in a file named joint_data.mat. There is one label for each of the three camera angles.\n",
    "\n",
    "Each 3-tuple is a point in the $ uvd $ coordinate space, which is the same as that used in the [depth images](#Depth-images). $ u $ and $ v $ are in pixels, while $ d $ is in millimeters.\n",
    "\n",
    "There are 36 tuples per label, corresponding to keypoints on the hand. Each finger is represented by 6 keypoints, 2 for each finger segment. Three points represent the palm and three represent the wrist.\n",
    "\n",
    "Finger keypoints are named as [finger]\\_[segment]\\_[component].\n",
    "\n",
    "Finger may be one of:\n",
    "\n",
    "* F1 (pinky)\n",
    "* F2 (ring)\n",
    "* F3 (middle)\n",
    "* F4 (index)\n",
    "* TH (thumb)\n",
    "\n",
    "Segment may be one of:\n",
    "\n",
    "* KNU3 (fingertip / distal phalanx)\n",
    "* KNU2 (finger middle / middle phalanx)\n",
    "* KNU1 (finger base / proximal phalanx)\n",
    "\n",
    "Component may be one of:\n",
    "\n",
    "* A (bone neck)\n",
    "* B (joint / bone base)\n",
    "\n",
    "Palm and wrist keypoints are named as follows:\n",
    "\n",
    "* PALM_1 (outer / ulnar wrist)\n",
    "* PALM_2 (inner / radial wrist)\n",
    "* PALM_3 (inner / radial palm)\n",
    "* PALM_4 (outer / ulnar palm)\n",
    "* PALM_5 (middle palm / ring metacarpal)\n",
    "* PALM_6 (middle wrist / lunate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_labels(set, angle):\n",
    "    joint_data  = sio.loadmat(data_path(set, labels_path))\n",
    "    labels      = joint_data['joint_uvd'][angle - 1]\n",
    "    joint_names = [name[0] for name in joint_data['joint_names'][0]]\n",
    "\n",
    "    return pd.Panel(labels, major_axis=joint_names, minor_axis=['u', 'v', 'd'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Depth images\n",
    "\n",
    "Depth images and synthetic depth images [store the most significant bits of depth data in the green channel and the least significant bits in the blue channel](https://github.com/jsupancic/deep_hand_pose/blob/master/src/caffe/jvl.cpp#L507)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_depth(image):\n",
    "    return np.expand_dims((image[:, :, 1] << 8) + image[:, :, 2], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output dataset\n",
    "\n",
    "The default [dataset](http://docs.h5py.org/en/latest/high/dataset.html) output directory is the dataset subfolder of the project root. The dataset is written to an [.hdf5](http://www.h5py.org/) file named dataset.hdf5.\n",
    "\n",
    "This file contains the following [groups](http://docs.h5py.org/en/latest/high/group.html):\n",
    "\n",
    "* image\n",
    "    * train\n",
    "    * test\n",
    "* label\n",
    "    * train\n",
    "    * test\n",
    "* pca\n",
    "    * eigenvectors\n",
    "    * mean\n",
    "    \n",
    "#### Images\n",
    "\n",
    "Each image is $ 128 \\times 128 \\times 1 $, containing depth data from the front angle, stored as single precision floating point numbers.\n",
    "\n",
    "#### Labels\n",
    "\n",
    "Each label is $ 28 \\times 1 $, containing the $ u $ and $ v $ values from the following hand keypoints, stored as single precision floating point numbers:\n",
    "\n",
    "* F1_KNU3_A\n",
    "* F1_KNU2_B\n",
    "* F2_KNU3_A\n",
    "* F2_KNU2_B\n",
    "* F3_KNU3_A\n",
    "* F3_KNU2_B\n",
    "* F4_KNU3_A\n",
    "* F4_KNU2_B\n",
    "* TH_KNU3_A\n",
    "* TH_KNU3_B\n",
    "* TH_KNU2_B\n",
    "* PALM_1\n",
    "* PALM_2\n",
    "* PALM_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASET_DIR = '../dataset'\n",
    "dataset     = h5py.File(path.join(DATASET_DIR, 'dataset.hdf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process(set):\n",
    "    images, labels = load_data(set, 'depth', 1)\n",
    "\n",
    "    data_image = dataset.require_dataset('image/' + set, (len(labels), 128, 128, 1), dtype='float')\n",
    "    data_label = dataset.require_dataset('label/' + set, (len(labels), 28), dtype='float')\n",
    "\n",
    "    p = Progbar(len(labels))\n",
    "\n",
    "    for image, item in zip(images, labels.iteritems()):\n",
    "        index, label = item\n",
    "        \n",
    "        image = convert_depth(image)\n",
    "        data_image[index], data_label[index] = normalize(image, label)\n",
    "        \n",
    "        p.update(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization\n",
    "\n",
    "The [Deep Hand Pose](https://github.com/jsupancic/deep_hand_pose) project performs a number of normalization steps on the NYU dataset before it is fed into the model.\n",
    "\n",
    "Broadly, these consist of:\n",
    "\n",
    "* Rescaling [image](https://github.com/jsupancic/deep_hand_pose/blob/master/src/caffe/jvl.cpp#L510)  and [label](https://github.com/jsupancic/deep_hand_pose/blob/master/src/caffe/jvl.cpp#L621) depth from millimeters to centimeters\n",
    "* [Recentering](https://github.com/jsupancic/deep_hand_pose/blob/master/src/caffe/jvl.cpp#L631) around the [middle finger base](https://github.com/jsupancic/deep_hand_pose/blob/master/src/caffe/jvl.cpp#L630)\n",
    "* [Clipping](https://github.com/jsupancic/deep_hand_pose/blob/master/src/caffe/jvl.cpp#L650) to [a smaller, depth-dependent bounding box](https://github.com/jsupancic/deep_hand_pose/blob/master/src/caffe/jvl.cpp#L638)\n",
    "* [Clamping](https://github.com/jsupancic/deep_hand_pose/blob/master/src/caffe/jvl.cpp#L665) depth data and scaling it to a range of -1 to 1\n",
    "* [Resizing](https://github.com/jsupancic/deep_hand_pose/blob/master/src/caffe/layers/hand_data_layer.cpp#L422) the image and label to $ 128 \\times 128 $\n",
    "* [Extracting a subset of the hand pose keypoints](https://github.com/jsupancic/deep_hand_pose/blob/master/include/jvl/blob_io.hpp#L8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(image, label):\n",
    "    label = label.copy()\n",
    "    \n",
    "    label.ix[:, 'd'] /= 10.\n",
    "    image = image / 10.\n",
    "    \n",
    "    center = label.ix['F3_KNU1_B', ['v', 'u', 'd']]\n",
    "    center.ix['d'] = label.ix[:, 'd'].median()\n",
    "\n",
    "    bounds = bounding_box(center, 525, 38).astype(int)\n",
    "    image, label = clip(image, label, bounds)\n",
    "    \n",
    "    image -= center.ix['d']\n",
    "    image = np.clip(image, -15, 15) / 15.0\n",
    "    image, label = resize(image, label, (128, 128))\n",
    "\n",
    "    image = np.expand_dims(image, 2)\n",
    "    label = label.ix[[\n",
    "            'F1_KNU3_A',\n",
    "            'F1_KNU2_B',\n",
    "            'F2_KNU3_A',\n",
    "            'F2_KNU2_B',\n",
    "            'F3_KNU3_A',\n",
    "            'F3_KNU2_B',\n",
    "            'F4_KNU3_A',\n",
    "            'F4_KNU2_B',\n",
    "            'TH_KNU3_A',\n",
    "            'TH_KNU3_B',\n",
    "            'TH_KNU2_B',\n",
    "            'PALM_1',\n",
    "            'PALM_2',\n",
    "            'PALM_3'\n",
    "        ], ['u', 'v']].values.flatten()\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get a bounding box of the specified dimensions in centimeters around\n",
    "# a point in uvd space\n",
    "def bounding_box(center, fx, size):\n",
    "    bounding_box = np.array([[0, 0], [1, 1]], dtype='float')\n",
    "    bounding_box -= 0.5\n",
    "    bounding_box *= size\n",
    "    bounding_box *= fx / center[-1]\n",
    "    bounding_box += center[:-1]\n",
    "    \n",
    "    return bounding_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Resize an image to the specified dimensions, scaling its label accordingly\n",
    "def resize(image, label, dimensions):\n",
    "    label.ix[:, ['v', 'u']] *= np.array(dimensions) / image.shape[:-1]\n",
    "    \n",
    "    # TODO: Try to implement or use OpenCV's INTER_AREA resize strategy?\n",
    "    image = scipy.misc.imresize(np.squeeze(image), dimensions, 'bilinear', mode='F')\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clip an image to the specified bounding box, translating its label accordingly\n",
    "# Bounding box should look like np.array([[x_1, y_1], [x_2, y_2]]), where\n",
    "# (x_1, y_1) are the coordinates of the lower left corner and \n",
    "# (x_2, y_2) are the coordinates of the upper right corner\n",
    "def clip(image, label, bounding_box):\n",
    "    label.ix[:, ['v', 'u']] -= bounding_box[0]\n",
    "    \n",
    "    image_box = np.array([[0, 0], image.shape[:-1]], dtype='int')\n",
    "    \n",
    "    padding = np.array([image_box[0] - bounding_box[0], bounding_box[1] - image_box[1]]).clip(0)\n",
    "    bounding_box += padding[0]\n",
    "    padding = np.concatenate((padding.T, np.array([[0, 0]])))\n",
    "    \n",
    "    image = np.pad(image, padding, 'edge')\n",
    "    image = image[slice(*bounding_box[:, 0]), slice(*bounding_box[:, 1])]\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "process('train')\n",
    "process('test')\n",
    "\n",
    "dataset.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
